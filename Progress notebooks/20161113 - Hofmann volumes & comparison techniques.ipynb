{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vols = pd.read_csv(\"volumes.csv\")\n",
    "vols = vols.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = pd.read_csv(\"temperatures.csv\")\n",
    "temps = temps.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#headers = [\"Ru\", \"Rb\", \"Rh\", \"Be\", \"Ba\", \"Bk\", \"Br\", \"H\", \"P\", \"Ge\", \"Gd\", \"Ga\", \"Pr\", \"Pu\", \"C\",\\\n",
    "#\"Pd\", \"Cd\", \"Ho\", \"Mg\", \"Mo\", \"Mn\", \"O\", \"S\", \"Eu\", \"Zr\", \"Er\", \"Ni\",\\\n",
    "#\"Na\", \"Nb\", \"Nd\", \"Fe\", \"B\", \"F\", \"Sr\", \"N\", \"Si\", \"Sn\", \"Sm\", \"V\", \"Sc\", \"Sb\", \"Se\", \"Co\",\\\n",
    "#\"Cl\", \"Ca\", \"Ce\", \"Xe\", \"Cs\", \"Cr\", \"Cu\", \"La\", \"Li\", \"Tm\", \"Ti\", \"Te\", \"Tb\", \"Tc\", \"Yb\", \"Dy\",\\\n",
    "#\"I\", \"Y\", \"Ag\", \"Al\", \"As\", \"In\"]\n",
    "\n",
    "#headers = [\"H\",\"Li\",\"Be\",\"B\",\"C\",\"N\",\"O\",\"F\",\"Na\",\"Mg\",\"Al\",\"Si\",\"P\",\"S\",\"Cl\",\\\n",
    "#\"K\",\"Ca\",\"Sc\",\"Ti\",\"V\",\"Cr\",\"Mn\",\"Fe\",\"Co\",\"Ni\",\"Cu\",\"Zn\",\"Ga\",\\\n",
    "#\"Ge\",\"As\",\"Se\",\"Br\",\"Rb\",\"Sr\",\"Y\",\"Zr\",\"Nb\",\"Mo\",\\\n",
    "#\"Tc\",\"Ru\",\"Rh\",\"Pd\",\"Ag\",\"Cd\",\"In\",\"Sn\",\\\n",
    "#\"Sb\",\"Te\",\"I\",\"Xe\",\"Cs\",\"Ba\",\"La\",\"Ce\",\"Pr\",\\\n",
    "#\"Nd\",\"Sm\",\"Eu\",\"Gd\",\"Tb\",\"Dy\",\"Ho\",\"Er\",\\\n",
    "#\"Tm\",\"Yb\",\"Lu\",\"Hf\",\"Ta\",\"W\",\"Re\",\"Os\",\"Ir\",\\\n",
    "#\"Pt\",\"Au\",\"Hg\",\"Tl\",\"Pb\",\"Bi\",\"Ac\",\"Th\",\"Pa\",\\\n",
    "#\"U\",\"Np\",\"Am\"]\n",
    "\n",
    "#headers = [\"Ru\", \"Rb\", \"Rh\", \"Be\", \"Ba\", \"Bi\", \"Br\", \"H\", \"P\", \"Ge\", \"Gd\", \"Ga\", \"Pr\", \"Pu\", \"C\",\\\n",
    "#\"Pd\", \"Cd\", \"Ho\", \"Mg\", \"Mo\", \"Mn\", \"O\", \"S\", \"Eu\", \"Zr\", \"Er\", \"Ni\",\\\n",
    "#\"Na\", \"Nb\", \"Nd\", \"Fe\", \"B\", \"F\", \"Sr\", \"N\", \"Si\", \"Sn\", \"Sm\", \"V\", \"Sc\", \"Sb\", \"Se\", \"Co\",\\\n",
    "#\"Cl\", \"Ca\", \"Ce\", \"Xe\", \"Cs\", \"Cr\", \"Cu\"]\n",
    "\n",
    "\n",
    "unitformula = pd.read_csv(\"scaledformulae.csv\")\n",
    "unitformula = unitformula.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, svm, kernel_ridge\n",
    "from scipy.optimize import nnls \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "regVols = linear_model.LinearRegression()\n",
    "regAlpha = linear_model.LinearRegression()\n",
    "#regVols = linear_model.ElasticNet(positive = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSD function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRMSD (y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "#w1 = [(3),(5),(10),(1)]\n",
    "#w2 = [(2),(4),(6),(8)]\n",
    "#getRMSD(w1,w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percCalc (row):\n",
    "    predictedvolume = row[\"sum\"]\n",
    "    vols = row[\"Volume\"]\n",
    "    percentage = predictedvolume / vols\n",
    "    if (percentage >=0.95) and (percentage <=1.05):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.00\n",
    "elements = unitformula.columns.values\n",
    "unitformula[\"Keep\"] = 1\n",
    "weights = []\n",
    "alphas =[]\n",
    "for chunk in zip(unitformula.groupby(np.arange(len(unitformula)) // 10000),vols.groupby(np.arange(len(vols)) // 10000),temps.groupby(np.arange(len(temps)) // 10000)):\n",
    "    print \"Chunk: \", chunk[0][0]\n",
    "    prevalpha = 1\n",
    "    RMSD = 10\n",
    "    chunkformula = chunk[0][1]\n",
    "    chunkvol = chunk[1][1]\n",
    "    chunktemp = chunk[2][1]\n",
    "    keepidx = chunkformula.index.tolist()\n",
    "    i = 0\n",
    "    while abs(prevalpha-alpha) > 0.00001 or (RMSD > 1) :\n",
    "        print \"Iteration:\", i\n",
    "        regVols.fit(chunkformula[chunkformula.Keep == 1], chunkvol.loc[keepidx].div(chunktemp.loc[keepidx].Temperature*alpha+1, axis=\"index\"))\n",
    "        if i > 0:\n",
    "            prevw = w#[0]\n",
    "            #print \"old w: \", prevw\n",
    "        w = regVols.coef_[0]\n",
    "        #w, rnorm = nnls(np.matrix(chunkformula[chunkformula.Keep == 1]),np.squeeze(np.array(chunkvol.loc[keepidx].div(chunktemp.loc[keepidx].Temperature*alpha+1, axis=\"index\"))))\n",
    "        if i > 0:\n",
    "            RMSD = getRMSD(prevw,w)#[0]\n",
    "            print \"New w\", zip(elements, w)\n",
    "            print \"RMSD: \", RMSD\n",
    "        x2=pd.DataFrame()\n",
    "        for line in zip(elements, w): #[0]\n",
    "            x2[line[0]] = chunkformula[line[0]].apply(lambda x: x*line[1])\n",
    "        x2[\"sum\"] = x2.sum(axis=1)\n",
    "        #if i>0:\n",
    "            #print x2[\"sum\"]\n",
    "        vol2 = chunkvol.loc[keepidx].div(x2[\"sum\"].loc[keepidx], axis=\"index\") - 1\n",
    "        regAlpha.fit(chunktemp.loc[keepidx], vol2)\n",
    "        if i > 0:\n",
    "            prevalpha = alpha\n",
    "            print \"Previous Alpha: \", prevalpha\n",
    "        alpha = regAlpha.coef_[0][0]\n",
    "        #alpha, rnorm2 = nnls(np.matrix(chunktemp.loc[keepidx]),np.squeeze(np.array(vol2)))\n",
    "        print \"Alpha: \", alpha\n",
    "        print \"deltaAlpha: \", abs(prevalpha-alpha)\n",
    "        #print \"dAlpha: \", abs(prevalpha - alpha)\n",
    "        chunkformula[\"Keep\"] = pd.concat([x2,vols], axis=1).apply(percCalc, axis=1)\n",
    "        keepidx = chunkformula.Keep[chunkformula.Keep == 1].index.tolist()\n",
    "        #print \"Element average volume (cubic angstroms)\", zip(elements,w[0])\n",
    "        i +=1\n",
    "        if (len(keepidx) <= 1) or (i > 200):\n",
    "            print \"Failure\"\n",
    "            break\n",
    "    weights.append(w)#[0])\n",
    "    alphas.append(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hofmw = np.array([37.3, 42, 31.2, 36, 13.24, 66, 32.7, 5.08, 29.5, 41.6, 56, 37.8, 57,\\\n",
    "13.87, 35, 51, 42, 36, 36, 38, 31.9, 11.39, 25.2, 39, 53, 27, 54, 26,\\\n",
    "26, 37, 50, 30.4, 13.24, 11.17, 47, 11.8, 37.3, 52.8, 50, 24, 42, 48, 30.3,\\\n",
    "29.4, 25.8, 45, 54, 45, 46, 28.1, 26.9, 58, 22.6, 49,\\\n",
    "27.3, 46.7, 45, 38, 59, 50, 46.2, 44, 35, 39.6, 36.4, 55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol2.hist(bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph of the volumes divided by the predicted volumes is made; this was just to test out making simple graphs and to view the distribution of volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for line in zip(elements, hofmw):\n",
    "            x2[line[0]] = chunkformula[line[0]].apply(lambda x: x*line[1])\n",
    "x2[\"sumhoff\"] = x2.sum(axis=1)\n",
    "print x2[\"sum\"]\n",
    "print x2[\"sumhoff\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Misc - Comparison methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the imported functions below are different methods of vector comparisons; these would tell us how similar two sets of values were, in this case it can be the predicted volumes and actual volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from scipy.spatial import distance\n",
    "cossimi = 1 - spatial.distance.cosine(x2[\"sumhoff\"], chunk[1][1])\n",
    "print cossimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the cosine similarity; it calculates the cosine of the angle between two n-dimensional vectors; what is expected is an angle of 0 which corresponds to a cosine sim value of 1. A value of 0 means values at 90 degrees to each other; and value of -1 means they are opposed.\n",
    "\n",
    "The value is subtracted from 1 because the function from scipy is (1 - cosine sim), therefore to revert it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eucsimi = spatial.distance.euclidean(x2[\"sumhoff\"], chunk[1][1])\n",
    "print eucsimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eucidean distance is basically the pythagoras distance, so the root of the sum of the differences squared is taken.\n",
    "$$\\sqrt{\\sum(A_{i} -B_{i})^{2}}$$\n",
    "\n",
    "This equation isn't entirely correct, the sum requires a i=1 and n limit, but I couldn't get it to work. This applies for all the equatiosn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mansimi = distance.minkowski(x2[\"sumhoff\"], chunk[1][1], 1) # manhattan distance \n",
    "chebsimi = distance.minkowski(x2[\"sumhoff\"], chunk[1][1], 3) # chebyshev distance\n",
    "print mansimi, chebsimi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manhattan distance = |x1 – x2| + |y1 – y2|.\n",
    "Chebyshev is as euclidean but the square and root power is set to an infinite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bray = 1 - distance.braycurtis(x2[\"sumhoff\"], chunk[1][1])\n",
    "print bray\n",
    "#bray curtis, similar to canberra, however here the weighting isn't absolute and also each component (numerator & denominator)\n",
    "#is summed before division (compared to canberra where it is summed after division.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bray curtis similarity is where the sum of the differences is divided by the sum of each set of values.\n",
    "A rough equation is shown below\n",
    "$$1 - \\frac{\\sum\\mid y_{i}-y_{j} \\mid}{\\sum(y_{i}+y_{j})}$$\n",
    "\n",
    "This is bound between 0 & 1; a higher value indicates more similarity therefore better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "canb = distance.canberra(x2[\"sumhoff\"], chunk[1][1])\n",
    "print canb\n",
    "#canberra, weighted form of manhattan distance (where the abs difference is divided by sum of absolutes of each set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The canberra similiarty is very similar to the bray curtis similiarty; however there is no max boundary; the minimum value is 0. Here a lower value indicates higher similiarty therefore is better.\n",
    "$$\\sum\\frac{\\mid y_{i}-y_{j} \\mid}{\\mid y_{i} \\mid + \\mid y_{j} \\mid}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = distance.correlation(x2[\"sumhoff\"], chunk[1][1])\n",
    "print corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance correlation basically shows how statistically independent two sets are from eachother, where 0 = totally independent and 1 is totally dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"The mean value of both our average volume: \", np.mean(w), \"and hofmann average volume: \", np.mean(hofmw)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python (2.7)",
   "language": "python",
   "name": "python2.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
